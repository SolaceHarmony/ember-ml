{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Wave Transformer Example with Ember ML\n",
    "\n",
    "This notebook demonstrates the structure and forward pass of a Wave Transformer model built using the Ember ML framework. Wave Transformers leverage attention mechanisms for processing sequential data, and this example showcases how Ember ML's backend-agnostic components can be assembled to create such architectures."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import numpy as np\n",
    "\n",
    "# Import Ember ML components\n",
    "from ember_ml.ops import set_backend\n",
    "from ember_ml.nn import tensor\n",
    "from ember_ml import ops\n",
    "from ember_ml.wave.models.wave_transformer import (\n",
    "    WaveMultiHeadAttention,\n",
    "    WaveTransformerEncoderLayer,\n",
    "    WaveTransformerEncoder,\n",
    "    WaveTransformer,\n",
    "    create_wave_transformer,\n",
    ")\n",
    "\n",
    "# Set a backend (choose 'numpy', 'torch', or 'mlx')\n",
    "# You can change this to see how the code runs on different backends\n",
    "set_backend('numpy')\n",
    "print(f\"Using backend: {ops.get_backend()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Generate Synthetic Sequence Data\n",
    "\n",
    "We will generate synthetic sequence data suitable for a transformer model. This data will have a batch dimension, a sequence length dimension, and a feature dimension (embedding dimension)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_dummy_sequence_data(batch_size, seq_length, embed_dim):\n",
    "    \"\"\"Creates dummy sequence data.\"\"\"\n",
    "    return tensor.random_normal((batch_size, seq_length, embed_dim), dtype=tensor.float32)\n",
    "\n",
    "# Define data parameters\n",
    "batch_size = 32\n",
    "seq_length = 50\n",
    "embed_dim = 64 # Feature dimension / Embedding dimension\n",
    "\n",
    "# Generate data\n",
    "input_sequence = create_dummy_sequence_data(batch_size, seq_length, embed_dim)\n",
    "\n",
    "print(f\"Input sequence shape: {tensor.shape(input_sequence)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Define the Wave Transformer Model\n",
    "\n",
    "We will define a Wave Transformer model, showcasing its key components: Multi-Head Attention and Transformer Encoder Layers. We can use the `create_wave_transformer` factory function for convenience."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define model parameters\n",
    "num_heads = 8\n",
    "ff_hidden_dim = 128 # Hidden dimension for the feed-forward network\n",
    "num_layers = 2 # Number of encoder layers\n",
    "\n",
    "# Create the Wave Transformer model using the factory function\n",
    "model = create_wave_transformer(\n",
    "    seq_length=seq_length,\n",
    "    embed_dim=embed_dim,\n",
    "    num_heads=num_heads,\n",
    "    ff_hidden_dim=ff_hidden_dim,\n",
    "    num_layers=num_layers,\n",
    ")\n",
    "\n",
    "print(\"Model Architecture:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Demonstrate Forward Pass\n",
    "\n",
    "We will pass the synthetic data through the Wave Transformer model to demonstrate a forward pass and observe the output shape."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Perform a forward pass\n",
    "# Note: Transformer models often require attention masks, but for simplicity,\n",
    "# we omit them in this basic forward pass demonstration.\n",
    "output = model(input_sequence)\n",
    "\n",
    "print(f\"Output sequence shape: {tensor.shape(output)}\")\n",
    "\n",
    "# The output shape should be the same as the input shape for a standard transformer encoder"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Notes on Training\n",
    "\n",
    "Training a complex model like a Transformer typically involves computing gradients and updating model parameters using an optimizer. While Ember ML provides optimizers (`ember_ml.training`) and the ability to compute gradients (`ops.gradients`), a full automatic differentiation system like `GradientTape` is required for seamless end-to-end training of complex, layered models. Without it, training would involve manual gradient calculations for each operation, which is complex for a Transformer.\n",
    "\n",
    "For supervised tasks with a Transformer, you would typically define a loss function (e.g., from `ember_ml.training`), compute the loss between model predictions and true labels, calculate gradients of the loss with respect to trainable parameters, and apply these gradients using an optimizer. The `ops.gradients` function can compute gradients for a given output with respect to specific inputs (like model parameters), which can be used to implement a manual training loop."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "\n",
    "This notebook provided a basic demonstration of the Wave Transformer model in Ember ML, focusing on its structure and forward pass. It showcased how Ember ML's backend-agnostic modules can be used to build advanced architectures. While full training requires a more complete automatic differentiation system, the core components for building and running the model are available and work across different backends."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
