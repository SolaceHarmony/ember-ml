#!/usr/bin/env python
"""
MLX high-precision math operations using double-double arithmetic.

This module provides float64 precision using two float32 values (hi, lo)
with error-free transformations based on Knuth's algorithms.

Key advantages over limb-based HPC16x8:
- Uses FMA (fused multiply-add) for exact error computation
- More efficient for basic arithmetic operations
- Single rounding point at final output
- Based on: Dekker (1971), Knuth TAOCP Vol 2, Shewchuk (1997)
"""
from typing import Optional

import mlx.core as mx

# ============================================================================
# Metal Header (shared across all kernels)
# ============================================================================
_HEADER = """#include <metal_stdlib>
using namespace metal;

// Two-Sum: Exact sum with error term (Knuth)
// Returns (s, e) where s = round(a + b) and e = (a + b) - s
inline float2 two_sum(float a, float b) {
    float s = a + b;
    float v = s - a;
    float e = (a - (s - v)) + (b - v);
    return float2(s, e);
}

// Quick Two-Sum: Assumes |a| >= |b| (faster)
inline float2 quick_two_sum(float a, float b) {
    float s = a + b;
    float e = b - (s - a);
    return float2(s, e);
}

// Two-Product: Exact product with error term using FMA
inline float2 two_prod(float a, float b) {
    float p = a * b;
    float e = fma(a, b, -p);  // Error: a*b - round(a*b)
    return float2(p, e);
}
"""

# ============================================================================
# Kernel Sources (bodies only - MLX generates signatures)
# ============================================================================

_DD_ADD_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    // Two-sum on high parts
    float2 s = two_sum(a_hi[gid], b_hi[gid]);

    // Two-sum on low parts
    float2 t = two_sum(a_lo[gid], b_lo[gid]);

    // Normalize: collect all error terms
    float s_lo = s.y + t.x;
    float2 result = quick_two_sum(s.x, s_lo);
    result.y += t.y;
    result = quick_two_sum(result.x, result.y);

    out_hi[gid] = result.x;
    out_lo[gid] = result.y;
"""

_DD_MUL_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    // Two-product on high parts (exact)
    float2 p = two_prod(a_hi[gid], b_hi[gid]);

    // Add cross terms: a_hi*b_lo + a_lo*b_hi
    float cross = a_hi[gid] * b_lo[gid] + a_lo[gid] * b_hi[gid];

    // Normalize
    float p_lo = p.y + cross;
    float2 result = quick_two_sum(p.x, p_lo);

    out_hi[gid] = result.x;
    out_lo[gid] = result.y;
"""

_DD_SUB_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    // Subtract by adding negation
    float2 s = two_sum(a_hi[gid], -b_hi[gid]);
    float2 t = two_sum(a_lo[gid], -b_lo[gid]);

    // Normalize
    float s_lo = s.y + t.x;
    float2 result = quick_two_sum(s.x, s_lo);
    result.y += t.y;
    result = quick_two_sum(result.x, result.y);

    out_hi[gid] = result.x;
    out_lo[gid] = result.y;
"""

_DD_DIV_SCALAR_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    // Quotient of high part
    float q = a_hi[gid] / b[gid];

    // Compute exact remainder using two_prod
    float2 p = two_prod(q, b[gid]);

    // Error term
    float e = (a_hi[gid] - p.x - p.y + a_lo[gid]) / b[gid];

    // Normalize
    float2 result = quick_two_sum(q, e);

    out_hi[gid] = result.x;
    out_lo[gid] = result.y;
"""

_DD_LIFT_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    out_hi[gid] = input[gid];
    out_lo[gid] = 0.0f;
"""

_DD_ROUND_SRC = r"""
    uint gid = thread_position_in_grid.x;
    if (gid >= length) return;

    // Final rounding happens here
    output[gid] = in_hi[gid] + in_lo[gid];
"""

# ============================================================================
# Lazy Kernel Compilation (compile once, cache globally)
# ============================================================================

_DD_ADD_KERNEL = None
_DD_MUL_KERNEL = None
_DD_SUB_KERNEL = None
_DD_DIV_SCALAR_KERNEL = None
_DD_LIFT_KERNEL = None
_DD_ROUND_KERNEL = None

def _get_add_kernel():
    """Compile add kernel on first use."""
    global _DD_ADD_KERNEL
    if _DD_ADD_KERNEL is None:
        _DD_ADD_KERNEL = mx.fast.metal_kernel(
            name="dd_add_kernel",
            header=_HEADER,
            source=_DD_ADD_SRC,
            input_names=["a_hi", "a_lo", "b_hi", "b_lo", "length"],
            output_names=["out_hi", "out_lo"],
            ensure_row_contiguous=True
        )
    return _DD_ADD_KERNEL

def _get_mul_kernel():
    """Compile multiply kernel on first use."""
    global _DD_MUL_KERNEL
    if _DD_MUL_KERNEL is None:
        _DD_MUL_KERNEL = mx.fast.metal_kernel(
            name="dd_mul_kernel",
            header=_HEADER,
            source=_DD_MUL_SRC,
            input_names=["a_hi", "a_lo", "b_hi", "b_lo", "length"],
            output_names=["out_hi", "out_lo"],
            ensure_row_contiguous=True
        )
    return _DD_MUL_KERNEL

def _get_sub_kernel():
    """Compile subtract kernel on first use."""
    global _DD_SUB_KERNEL
    if _DD_SUB_KERNEL is None:
        _DD_SUB_KERNEL = mx.fast.metal_kernel(
            name="dd_sub_kernel",
            header=_HEADER,
            source=_DD_SUB_SRC,
            input_names=["a_hi", "a_lo", "b_hi", "b_lo", "length"],
            output_names=["out_hi", "out_lo"],
            ensure_row_contiguous=True
        )
    return _DD_SUB_KERNEL

def _get_div_scalar_kernel():
    """Compile division kernel on first use."""
    global _DD_DIV_SCALAR_KERNEL
    if _DD_DIV_SCALAR_KERNEL is None:
        _DD_DIV_SCALAR_KERNEL = mx.fast.metal_kernel(
            name="dd_div_scalar_kernel",
            header=_HEADER,
            source=_DD_DIV_SCALAR_SRC,
            input_names=["a_hi", "a_lo", "b", "length"],
            output_names=["out_hi", "out_lo"],
            ensure_row_contiguous=True
        )
    return _DD_DIV_SCALAR_KERNEL

def _get_lift_kernel():
    """Compile lift kernel on first use."""
    global _DD_LIFT_KERNEL
    if _DD_LIFT_KERNEL is None:
        _DD_LIFT_KERNEL = mx.fast.metal_kernel(
            name="dd_lift_kernel",
            header=_HEADER,
            source=_DD_LIFT_SRC,
            input_names=["input", "length"],
            output_names=["out_hi", "out_lo"],
            ensure_row_contiguous=True
        )
    return _DD_LIFT_KERNEL

def _get_round_kernel():
    """Compile round kernel on first use."""
    global _DD_ROUND_KERNEL
    if _DD_ROUND_KERNEL is None:
        _DD_ROUND_KERNEL = mx.fast.metal_kernel(
            name="dd_round_kernel",
            header=_HEADER,
            source=_DD_ROUND_SRC,
            input_names=["in_hi", "in_lo", "length"],
            output_names=["output"],
            ensure_row_contiguous=True
        )
    return _DD_ROUND_KERNEL

# ============================================================================
# Python API: Double-Double Class
# ============================================================================

class Float64:
    """
    Float64 precision using double-double arithmetic.

    Internally stores two float32 values (hi, lo) where:
    - hi contains the primary value
    - lo contains the error correction term
    - Together they provide ~30-32 decimal digits of precision

    All operations use error-free transformations to maintain precision
    until the final rounding point.
    """

    def __init__(self, hi: mx.array, lo: Optional[mx.array] = None):
        """Initialize Float64 from high and low components."""
        self.hi = mx.array(hi, dtype=mx.float32)
        if lo is None:
            self.lo = mx.zeros_like(self.hi)
        else:
            self.lo = mx.array(lo, dtype=mx.float32)

    @classmethod
    def from_float32(cls, value: mx.array) -> 'Float64':
        """Lift float32 to Float64 (hi=value, lo=0)."""
        value = mx.array(value, dtype=mx.float32)
        length = mx.array(value.size, dtype=mx.uint32)

        kernel = _get_lift_kernel()
        hi, lo = kernel(
            inputs=[value.reshape(-1), length],
            output_shapes=[value.shape, value.shape],
            output_dtypes=[mx.float32, mx.float32],
            grid=(value.size, 1, 1),
            threadgroup=(min(256, value.size), 1, 1)
        )
        return cls(hi, lo)

    def to_float32(self) -> mx.array:
        """Round to float32 (SINGLE ROUNDING POINT)."""
        length = mx.array(self.hi.size, dtype=mx.uint32)

        kernel = _get_round_kernel()
        output, = kernel(
            inputs=[self.hi.reshape(-1), self.lo.reshape(-1), length],
            output_shapes=[self.hi.shape],
            output_dtypes=[mx.float32],
            grid=(self.hi.size, 1, 1),
            threadgroup=(min(256, self.hi.size), 1, 1)
        )
        return output

    def add(self, other: 'Float64') -> 'Float64':
        """Add two Float64 values using error-free transformation."""
        length = mx.array(self.hi.size, dtype=mx.uint32)

        kernel = _get_add_kernel()
        hi, lo = kernel(
            inputs=[
                self.hi.reshape(-1), self.lo.reshape(-1),
                other.hi.reshape(-1), other.lo.reshape(-1),
                length
            ],
            output_shapes=[self.hi.shape, self.hi.shape],
            output_dtypes=[mx.float32, mx.float32],
            grid=(self.hi.size, 1, 1),
            threadgroup=(min(256, self.hi.size), 1, 1)
        )
        return Float64(hi, lo)

    def subtract(self, other: 'Float64') -> 'Float64':
        """Subtract two Float64 values."""
        length = mx.array(self.hi.size, dtype=mx.uint32)

        kernel = _get_sub_kernel()
        hi, lo = kernel(
            inputs=[
                self.hi.reshape(-1), self.lo.reshape(-1),
                other.hi.reshape(-1), other.lo.reshape(-1),
                length
            ],
            output_shapes=[self.hi.shape, self.hi.shape],
            output_dtypes=[mx.float32, mx.float32],
            grid=(self.hi.size, 1, 1),
            threadgroup=(min(256, self.hi.size), 1, 1)
        )
        return Float64(hi, lo)

    def multiply(self, other: 'Float64') -> 'Float64':
        """Multiply two Float64 values using FMA-based two_prod."""
        length = mx.array(self.hi.size, dtype=mx.uint32)

        kernel = _get_mul_kernel()
        hi, lo = kernel(
            inputs=[
                self.hi.reshape(-1), self.lo.reshape(-1),
                other.hi.reshape(-1), other.lo.reshape(-1),
                length
            ],
            output_shapes=[self.hi.shape, self.hi.shape],
            output_dtypes=[mx.float32, mx.float32],
            grid=(self.hi.size, 1, 1),
            threadgroup=(min(256, self.hi.size), 1, 1)
        )
        return Float64(hi, lo)

    def divide_scalar(self, scalar: mx.array) -> 'Float64':
        """Divide Float64 by scalar float32."""
        scalar = mx.array(scalar, dtype=mx.float32)
        if scalar.size == 1:
            scalar = mx.broadcast_to(scalar, self.hi.shape)

        length = mx.array(self.hi.size, dtype=mx.uint32)

        kernel = _get_div_scalar_kernel()
        hi, lo = kernel(
            inputs=[
                self.hi.reshape(-1), self.lo.reshape(-1),
                scalar.reshape(-1), length
            ],
            output_shapes=[self.hi.shape, self.hi.shape],
            output_dtypes=[mx.float32, mx.float32],
            grid=(self.hi.size, 1, 1),
            threadgroup=(min(256, self.hi.size), 1, 1)
        )
        return Float64(hi, lo)

    # Operator overloading
    def __add__(self, other):
        if isinstance(other, Float64):
            return self.add(other)
        else:
            return self.add(Float64.from_float32(mx.array(other)))

    def __sub__(self, other):
        if isinstance(other, Float64):
            return self.subtract(other)
        else:
            return self.subtract(Float64.from_float32(mx.array(other)))

    def __mul__(self, other):
        if isinstance(other, Float64):
            return self.multiply(other)
        else:
            return self.multiply(Float64.from_float32(mx.array(other)))

    def __truediv__(self, other):
        if isinstance(other, (int, float, mx.array)):
            return self.divide_scalar(mx.array(other))
        else:
            raise NotImplementedError("Full Float64 / Float64 not yet implemented")

    def __repr__(self):
        return f"Float64(hi={self.hi}, lo={self.lo})"

# ============================================================================
# Convenience Functions
# ============================================================================

def add(a: mx.array, b: mx.array) -> mx.array:
    """
    Add two arrays in Float64 precision, returning float32.

    This performs a + b with NO intermediate rounding.
    Rounding happens only at the final output.
    """
    a_dd = Float64.from_float32(a)
    b_dd = Float64.from_float32(b)
    result_dd = a_dd.add(b_dd)
    return result_dd.to_float32()

def multiply(a: mx.array, b: mx.array) -> mx.array:
    """
    Multiply two arrays in Float64 precision, returning float32.

    Uses FMA-based two_prod for exact error computation.
    Rounding happens only at the final output.
    """
    a_dd = Float64.from_float32(a)
    b_dd = Float64.from_float32(b)
    result_dd = a_dd.multiply(b_dd)
    return result_dd.to_float32()

def subtract(a: mx.array, b: mx.array) -> mx.array:
    """
    Subtract two arrays in Float64 precision, returning float32.

    Rounding happens only at the final output.
    """
    a_dd = Float64.from_float32(a)
    b_dd = Float64.from_float32(b)
    result_dd = a_dd.subtract(b_dd)
    return result_dd.to_float32()

def divide_scalar(a: mx.array, b: float) -> mx.array:
    """
    Divide array by scalar in Float64 precision, returning float32.

    Rounding happens only at the final output.
    """
    a_dd = Float64.from_float32(a)
    result_dd = a_dd.divide_scalar(mx.array(b))
    return result_dd.to_float32()

# ============================================================================
# Testing
# ============================================================================

if __name__ == "__main__":
    print("=" * 80)
    print("Float64 Double-Double Arithmetic Test")
    print("=" * 80)

    # Test 1: Simple addition
    print("\n--- Test 1: Addition ---")
    a = mx.array([1.0, 2.0, 3.0], dtype=mx.float32)
    b = mx.array([0.1, 0.2, 0.3], dtype=mx.float32)
    result = add(a, b)
    print(f"a = {a}")
    print(f"b = {b}")
    print(f"a + b = {result}")

    # Test 2: Multiplication
    print("\n--- Test 2: Multiplication ---")
    a = mx.array([2.0, 3.0, 4.0], dtype=mx.float32)
    b = mx.array([1.5, 2.5, 3.5], dtype=mx.float32)
    result = multiply(a, b)
    print(f"a = {a}")
    print(f"b = {b}")
    print(f"a * b = {result}")

    # Test 3: Subtraction
    print("\n--- Test 3: Subtraction ---")
    a = mx.array([10.0, 20.0, 30.0], dtype=mx.float32)
    b = mx.array([0.1, 0.2, 0.3], dtype=mx.float32)
    result = subtract(a, b)
    print(f"a = {a}")
    print(f"b = {b}")
    print(f"a - b = {result}")

    # Test 4: Division by scalar
    print("\n--- Test 4: Division by Scalar ---")
    a = mx.array([1.0, 2.0, 3.0], dtype=mx.float32)
    b = 3.0
    result = divide_scalar(a, b)
    print(f"a = {a}")
    print(f"b = {b}")
    print(f"a / b = {result}")

    # Test 5: Using Float64 class directly
    print("\n--- Test 5: Float64 Class ---")
    a = Float64.from_float32(mx.array([1.0, 2.0]))
    b = Float64.from_float32(mx.array([3.0, 4.0]))
    c = a + b
    print(f"a = Float64(hi={a.hi}, lo={a.lo})")
    print(f"b = Float64(hi={b.hi}, lo={b.lo})")
    print(f"c = a + b = Float64(hi={c.hi}, lo={c.lo})")
    print(f"c.to_float32() = {c.to_float32()}")

    print("\n" + "=" * 80)
    print("All tests completed âœ“")
    print("=" * 80)
